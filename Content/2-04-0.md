## De-biasing Algorithms and fairness

*Fairness-aware machine learning* is a term used in an EU report [@Tolan2018] that asserts that *fairness* in this sense depends on the domain in which the model is being made and therefore the fairness constraints applied should be specific to that domain.

The group fairness approaches in the EU report apply political or popular biases like: 

> "the share of defendants classified as high risk should be equal across different protected groups" -- @Tolan2018, p8

This is called *demographic parity* and tries to neutralise an aspect of the source data: suppressing one truth in the name of another, perhaps. In another, *calibration* is applied such that "the proportion of people re-offending is the same across protected groups" (ibid, p.10), and to achive *similar people should be treated similarly* is considered a non-trivial task on account of deciding on what data is required to identify how similar two people are.

"Fairness through unawareness" is one method described by which algorithms might be made to mitigate bias: this seems to take us full circle back to where the problem began, in which the use of historical data is used to train the model from the prior behaviours which themselves have included hidden biases.

## Data Justice

The framework called Data Justice looks at both social and technical aspects of machine bias. The idea is to oppose exacerbating social injustice by the rapid adoption of technologies that embed data which itself may include past social injustice. The course points us again at @Couchman2019 for its stance on "policing by machine" but also introduces us to one writer's proposal for redressing the power balance back in favour of the citizen, namely (in)visibility, (dis)engagement with technology and antidiscrimination [@Taylor2017].

The first of these calls for greater transparency of what and how data is collected and used, and allows individuals to choose not to be part of it. The second relates to the latter point and calls for greater control for the individual on how (or whether) they participate in the data markets. Finally, individuals should have the right to call out bias or unfair treatment at the hands of data-informed systems. This, I think is particularly important for public services.

Increasingly, watchdog groups are being established around the world to raise awareness and facilitate action against the abuse of data. The UK's [Data Justice Lab](https://datajusticelab.org/) sits within Cardiff University's media school. Interstingly, the [FAT/ML](https://www.fatml.org/)[^fatml] website hasn't been updated for the past two years, which might suggest that nothing much is happening in the group. 

[^fatml]: Fairness, Accountability and Transparency in Machine Learning, led by Solon Barocas, of @Barocas2014 and Microsoft.
