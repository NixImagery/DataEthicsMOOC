## Bias

The course resorts to Webster's, of all places, to find a definition of bias. My background understands the term as:

> "A steady voltage or current applied to an electronic device" -- [@OED-bias]

For me, bias and its sister, discrimination are not inherently Bad Things. The ordering of the meanings of the word in these dictionaries offers perhaps a cultural explanation for a bias of the course writers, who seem to be seeking to associate injustice with this word. The OED's principal meaning of *bias* is *slanting, oblique* [@OED-bias].

### Algorithmic bias
One problem with the current criticism of "algorithmic bias" is that the scapegoat is either the algorithm itself, or the programmers that make it. It seems unfashionable to respond, "fair comment" when outcomes of data analysis go against our modern, fragile, sensibilities:

> "Googleâ€™s ad-serving system showed an ad for high-paying jobs to men much more often than it did for women" -- from CMU research, cited in @Kirkpatrick2016

Clearly the targeting of the ad wasn't based upon the single variable of gender: other factors are significant in the selection *and are ignored in the reporting* to make a more sensational commentary. This is the real difficulty for me here: that we ignore the manipulation of our responses by irresponsible and lucrative[^exciteme] articles like these.

[^exciteme]: We like to be titillated with stories like this: we buy newspapers for them, we watch the channels that serve them, and subscribe to media that feeds them to us.


### Big Data and implicit bias

A reading from this week [@Barocas2014] also examines the meaning of words when suggesting ways of tackling the problem of implicit bias in algorithms derived from historical data. I like the moderate language used in this essay:

> "If data miners are not careful, the process can result in disproportionately adverse outcomes concentrated within historically disadvantaged groups in ways that look a lot like discrimination." -- @Barocas2014, p. 673

> "Discrimination may be an artifact of the data mining process itself, rather than a result of programmers assigning certain factors inappropriate weight" -- ibid. p. 674

So, we can step away from the tribal frowning at the technology and the people who program it and recognise that our own past behaviours have created this learned behaviour, in the same way that it is created in our own attitudes and biases. @Barocas2014 suggests this can be done via the lens of (US) Title VII [-@TitleVii1964] and by doing what I consider to be the obvious, which is to understand how these algorithms get their biases in the first place:

> "Data mining takes the existing state of the world as a given and ranks candidates according to their predicted attributes in *that* world." -- @Barocas2014, p. 731, orginal emphasis
