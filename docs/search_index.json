[["index.html", "Data Ethics, AI and Responsible Innovation Introduction", " Data Ethics, AI and Responsible Innovation Nick Hood 10 November 2020 Introduction These are my notes from participation in the Edinburgh Data Ethics MOOC, running in November and December 2020. Any errors here are my own responsibility. Nick Hood @NixImagery This document was last updated on 10 November 2020 at 09:36. "],["course-overview.html", "Course overview", " Course overview Week 1 Ethics and Law What are ethical values? Can we rely on legal regulation? What are the most pressing issues facing data driven industries? Week 2 Crime and Justice Should we use predictive policing and sentencing algorithms? How can biases sneak into such algorithms? How can we remove them? Week 3 Home and City What are the promises of smart homes and cities? How can they impact our privacy and freedom? How can we design them to protect those values? Week 4 Money and Markets Can future tech lead to a world without money? Can algorithms help us fairly distribute resources? How could we design a fair AI? Week 5 Life and Health Should we keep genetic databanks? Would you trust an AI doctor? What are the principles of responsible research and innovation? "],["week-1-ethics-and-law.html", "Week 1: Ethics and Law", " Week 1: Ethics and Law This starts with some housekeeping, introductions to the tutors and setting up a profile within the course. Dr. Ewa Luger gets us started by considering a “broad overview” of ethics, using a radio interview about the Cambridge Analytica (CA) scandal as a resource to get us thinking before we embark on a more detailed introduction to ethics. Ethical ‘Issues’ Introduction to Ethics Legal and ethical Considerations Information, Control and Power Ethical ‘Issues’ In the clip, problematised issues with the CA matter included: the way data was gathered from people the fact that the data was then passed on to another party that the data was now available for commercial use that the data was used for political purpose the FaceBook (FB) login on an app gave permissions for all of users’ FB data to be used and shared people aren’t outraged, or even bothered by this enough to take action Discussion forum task Think back over your professional life and identify one example of an ethical issue that you personally experienced. In the discussion forum, write a brief summary (around 50-100 words) describing this issue - try to explain the context, what happened, and why you felt it was an ethical issue. After you’ve done this, comment on two other posts. I had to take a quick look at the forum first to gauge the level of ethical matters that were being shared. There seems to be a discernible difference between moral, legal and ethical matters in the revelations on the forum. My own example came from quite a selection of professional experience that ranged from legal but immoral, moral but illegal, unethical but moral and legal. I picked an example from a while back. I was project manager in a business that made flight crew training simulators. These devices were multi-million-dollar complex machines that used specially designed and manufactured circuit boards mounted in standard racks to drive the various components of the device. I had an internal review one morning that failed badly when the equipment that had been working well the night before suddenly failed to function. It was extremely embarrassing for my team. We discovered later that day that in the night, another project manager had switched a key circuit board for a faulty one from his own machine. The other examples I commented on or read seemed to me to be not so much ethical issues as difficult choices or situations. This is causing me to wonder if I have a secure understanding of the term, “ethical”. Hopefully the next section will clear that up for me. "],["the-challenge-of-data-driven-innovation.html", "The challenge of data-driven innovation", " The challenge of data-driven innovation Issues arising from the 2018 Cambridge Analaytica scandal are identified as: the value derived from data is not evenly distributed power rests in the hands of very few the potential for harm and inequality is high the potential impact of bad actors These are, in my view, issues of power arising from wealth inequity and not particularly related to anything technological. The difficulty is that technology amplifies the effects (as it amplifies inequities in a classroom – this much we have learned in moving to “Digital First” pedagogies resulting from the COVID measures). The basic principles of ethics are described as, “how we should address these issues, whilst minimising harm, ensuring morally good outcomes and fairness, and protecting human autonomy.” Data at the heart of it For the purposes of the course, this definition of ethics is set in the context of data-driven innovation which makes use of large amounts of data to train algorithms that make predictions or provide insights for decision making. Machines are increasingly now involved in the decision making too, and in taking actions. These machines are described as intelligent. Visibility of rationale These machines include, of course, neural networks and we know from the way that these pattern-matchers work, that other than the input and output layers, it is not possible to determine the path of the rationale – the specific choices ot selections being made in the intermediate steps – that underlies the outputs. Ambient intelligent systems Interactional moments are perceived to provide friction in the operation of systems, therefore designers are removing these for a smoother experience – think of entering your password every time you wanted to look at your phone, now replaced with fingerprint or face recognition. The result is that such interactions become invisible to the user: it is the environment we interact with in a natural way. Unnaturally, the environment is reading us to discern intent. Human –&gt; AI interaction This is problematised in a video clip, suggesting that this interaction is not yet defined. I rather think that Turing (1950) had a clear enough view of what it might look like, but recent innovations have increased system opacity such that understanding how the system makes choices based upon how it perceives us is difficult. In some sense, we are already familiar with this idea with our less tech-savvy relatives who need support working with machines. It gets harder when you don’t know there’s a machine. Utility Comparison with basic utilities like water, power, etc., is made from a very privileged first-world perspective: we only notice them when they fail. These systems were unregulated in their early days, too, and some of the horrors of human behaviour (Edison, for example) are long forgotten. As data-driven innovation moves to infect our lives by becoming a utility, the question is asked, “should it be regulated?” First steps have been made in this, of course, and we see the return of interactional friction with things like the cookie quiz on GDPR-compliant websites. The monetisation of data has led to the term, “surveillance capitalism”, which describes how our identity and behaviours have become the product in a high-value industry. Policy lag Clearly, policy makers are well behind the curve when it comes to data-driven innovation. GDPR and similar legislation have added friction yet advances are made at speed. The additional reading (Wagner, 2019) is critical of policy makers and suggests that ethics is what corporations do to avoid government interference. Values What we value in society is a cultural attribute. Values are therefore dependent on context and are not absolute: I have often said that every principle has a price attached to it, and that if the price is right, all principles are for sale. This has caused strong reaction when vocalised that way, especially from those who feel that they themselves hold high principles in comparison with others. Clearly, passions are aroused when deeply-held values are challenged, as the Guardian article on Peter Seeger illustrates. Examples are discussed in the course material to illustrate the point that values depend very much on context. This statement is a call to action, perhaps, talking of moral dilemmas and conflicting values: “Our job, as ethical actors, is to identify what they are, and then negotiate how best to instantiate and balance them.” Research is as robust as you’d expect for the social sciences, with this, from a study of a small sample of people (N &lt; 700) in three countries with similar (colonial) cultural pedigree: “… people endorse the same values to a similar extent across countries and also instantiate them similarly.” (Bardi et al., 2018) Not much of a surprise there, given the evolutionary path of the handful of people studied. Case studies Proejct Maven and the Google employee revolt against it is presented as the first case study of how corporations can make choices that do not align with the values of the stakeholders, in this case, the staff. That project sat in stark contrast to Google’s “Don’t be evil” mantra, still enshrined in its code of conduct (cf. the characterisation of ethics as an escape from regulation by Wagner (2019)). The military application of data-driven innovation is clearly compelling, and others are emerging, including Clearview AI’s face recognition app, which the company has worked hard to present as ethical use of technology. References "],["what-are-ethics.html", "What are ethics?", " What are ethics? A branch of moral philosophy: a codification of habits that are valued within the context of a culture: ergo, no right or wrong answers, just human custom. Normative ethics are those that define what we ought to do, without concerning themselves with what we actually do. Within that class of ethics are 3 persepctives on how we should act: Deontological ethics (rules) rules of duty and obligation – perhaps universally agreed upon, like not killing each other, or hurting animals. Some things are always right, others, always wrong, no matter the consequences. Teleological ethics (consequences) focus on the outcome of actions – actions for the greater good, perhaps, or the end justifying the means. Consequentialism is the view that the moral quality of a choice is decided solely by its outcome. Virtue ethics are about judgement of people’s character or moral fibre. The obvious issue here is that life is complicated, isn’t it? We can’t make a simple set of rules because the complexity of life requires us to make choices sometimes that go against those simple rules for a better end. Is there any point, then, in having rules in the first place? Well, clearly, because we want everyone else to make choices that don’t harm or disadvantage us. The trolley problem Scenario 1 A runaway train is travelling on a railway track towards 5 people. You can’t warn them, but there is a lever that you can operate to switch the tracks. The problem is that there is a person on the other track. Do you pull the lever or not? Scenario 2 As above, but now there are no points – instead, a large person standing a bridge over the tracks. Do you push Fatty into the path of the train to save the five? I chose yes in both scenarios. “The needs of the many outweigh the needs of the few.” – Spock Interstingly, although I was with the majority in the first scenario, I was in the minority in the second. I don’t understand this difference, except for the difference between operating a control like the lever, and physically acting on another human being – the former seems less connected to the action, perhaps. Other scenarios How does the trolley problem change when the 5 are convicted rapists? Children? When Fatty is a scientist working on a cure for Leukemia? When the 5 are Mountain People (insert your own other-class of person)? Tories? Welsh? Rednecks? Making moral decisions So, are ethics about choices, or outcomes? Rules or consequences? MIT are trying to build a picture of moral acceptability with their crowdsourcing moral machine project in which you are presented with multiple scenarios (like driverless car choices) in which you decide the path the vehicle should take, usually resulting in somebody or something dying. It is easy to become quickly abstracted from the awfulness of the choices you make in this game. Guidelines in ethical frameworks for machine choices are broadly divided into four categories: Do good Minimise harm Respect human autonomy Be just or fair Someone should tell Peugeot about the third one. My wife’s 2008 has a really irritating habit of grabbing the wheel if you change lanes without indicating, thinking that you’ve fallen asleep. I haven’t found a way of switching that off yet, but it is really disconcerting, especially on a long trip at night with no other vehicles around. Transparency of the algorithm and accountability are increasingly being emphasised. These frameworks have become regarded as inadequate as they offer a way for corporations to hide behind them in what is called, “ethics washing”. This is a problem for all rules or specifications, or checklists. Decision-making in Scottish Teacher Education In my own application, teacher education1, the GTCS Professional Standards are meant to provide an objective benchmark that describes the competencies and skills of all teachers in Scotland. We know that the application of these standards is in the hands of the profession itself and therefore wildly variable in their interpretation. The standards themselves are written in ambiguous and wishy-washy language, like most things in state-provisioned education, and so are highly subjective and open to – interpretation or abuse, depending on your view of an individual situation. “Universal law is for lackeys. Context is for kings.” – Captain Lorca, StarTrek Discovery Moral decision-making applied to data ethics A couple of questions are asked in a section making the links between moral decision making and data ethics. The first, “Should we require people to give their DNA to a gene bank?”, screams at me, perhaps because of my age and the closeness people of my age have, although not by first-hand experience, of the horrors of the Second World War. Everything about centralised reporting of ethnicity makes me recoil: I never provide information about my ethnicity, and I would need a very good reason before I ever did. We are forgetting: something that gets me in trouble almost every year, with very real regret and a deepening sense of injustice2. The second question relates to social media and their handling of “hate speech”. This is topical: I have just deleted my personal Facebook account3 after they, once again, dismissed my objections to posts intended to whip up hatred of muslims, or other racial or ethnic groupings. In every case I have raised, the posts have not been found to breach codes of conduct, illustrating what is described as ethical washing (see above). To my cost – I have cut off friends and family with whom I am only connected this way – I have deleted my account. A thought experiment We are tasked with making our own thought experiment to allows us to examine our beliefs and “surface” factors that influence our judgement. This is mine. The question Is it OK to kill to save lives in a war when you are not a combatant? Parameters In the context of war, it is given that the combatants of one side are allowed to kill the combatants of the other. What constitutes “combatant”, however? Is it anyone who wears the uniform, or stands behind the barricades? What about observers? The story During an African war, a soldier of an impartial peacekeeping force (soldier A) is invited to take a ride in a helicopter on a routine supply drop to a station in the bush. Soldier A has not been explicitly told that he is not allowed to ride in the vehicles of either side. The route is a well-known safe corridor, well policed by peacekeepers and resepcted by both sides in the war. Soldier A takes the ride, seated in the open side door of the helicopter next to a mounted cannon. En route, the helicopter comes under fire unexpectedly. The pilot takes evasive action by banking sharply to right. Soldier A can see smoke coming from a group of boulders on the ground directly in front of him. He realises that his life, and the lives of the others in the helicopter are in danger. He hears the pilot on the intercom shouting, “Shoot them! Shoot them!” You are soldier A. Do you use the cannon and try to save yourself and the crew? We don’t like “teacher training” because we like to think that teaching is a profession, like the law, or the military. It isn’t, of course, but we sustain the pretence for ourselves, even if nobody else in society beleives it.↩︎ When I call people out for their disrepect of the remembrance observation, it is always me that is critiqued for not being kind, or collegiate. People can be so ungrateful and selfish. The injustice of such treatment makes me wonder.↩︎ I am keeping, uncomfortably, a couple of social media accounts going that are related to my media and technical interests. Incidentally, I also deleted my personal Twitter account, but not for this reason. I’m generally a bit fed up with the whole Internet at the moment.↩︎ "],["legal-and-ethical-considerations.html", "Legal and ethical considerations", " Legal and ethical considerations Legal Well, let’s not pretend that there’s much of a connection between law and morality. Laws are instruments of power. They belong to the powerful. Even at mate’s rates, a lawyer’s fees are vulgar and shameless. Even if it were accessible by the common people, legal decisions have always been historically hysterical, wildly inconsistent and woefully inadequate. It’s noticeable that the great democratisers of the law have been people like Napolen and Hitler. It’s about time we replaced the whole system with a machine. “The first thing we do, let’s kill all the lawyers.” – Henry VI, Part 2, Act IV, Scene 2 (1591) The laws of the land are the first and greatest example of what the course describes as “ethics washing”, the practice of corporations who create “codes” to lure the public into a false sense of security and earn undeserved trust. Laws make us feel safe but do nothing to make us safe as enforcement and monitoring are lacking. In the same way that the discussion in the course speaks of ethics guidelines being used by companies “to ward off regulation by the state through formal laws”, so government uses laws to ward off revolution and disorder. There is a tension in data ethics between the need and cost of self-regulatory codes of conduct, and external regulation through the courts. The course summarises data ethics and the law: Hard ethics is needed to understand and interpret laws, and to make sure legislation is followed. Soft ethics are normative rules that tell us how to behave morally, whether the law addresses our actions or context. Compliance with the law is normally necessary for ethically correct conduct, but may not be enough for it. Ethical? Nothing ethical going on here. I skimmed the transcript of the role-play presented by the course (I am out of week already) and also the discussion on the ethics of setting cookies for the participants of ths MOOC. Much angst, I see. Tracking is an issue for me and therefore I take steps to mitigate it. I don’t allow setting of third-party cookies, and have a policy of clearing all cookies on exit from a browser session. I also use different browsers. Tracking is difficult under those circumstances: this is working for me. How do I know? When I get advertising, it’s usually totally irrelevant, which gives me some comfort, but I’m not complacent about it. I hate advertising: it’s one of the manifestations of what is truly awful about human beings. Solutions like the GDPR persmissions dialogues are no better than the advertising itself. They doesn’t solve any problems, they just gets in the way and decrease the chances of me using the sites that push them into my face. I make use of ad blockers, brutal cookie policy, and text readers when using the web. Interestingly, the link to the EC directive yields a 404 (not found) error. The EdX FAQ is a good example of the dishonest corporate response to such issues: it presents questions “frequently asked” by users which it doesn’t answer. One of the difficulties of this dominance of the common habit of making a website, for whatever reason, of the legal constraints is that the people that benefit most from this are lawyers. They get fat on the friction of common activities. I have been making websites since the early 1980s before anyone knew what the WorldWide Web was. Most of them are pro bono educational sites, or services for the common good. Some have been for profit but not in the “YouTuber” sense: I have charged fees to compensate for the time it has taken. That time has not included exhaustive compliance checking with the various laws and expectations. I have always worked on the basis that I haven’t got any money, so am not worth pursuing in litigation. So far, so good. At no point has it been made clear why cookies are necessary. Disabling cookies on the EdX site results in denial of access immediately on refreshing the page. So, the first purpose is to grant access to the site (which can be done in other ways easily enough, and more securely). References "],["information-control-and-power.html", "Information, control and power", " Information, control and power Formalism Legal choices based on logic only: what you did, and what the law is. Realism Your conviction depends to a large extent on what the judge ate for breakfast. A task is given in which students are asked to decide if building an app to influence proceedings based upon what is known from an AI analysis of jury members, is ethical. I picked the red envelope, but both answers are characterised as “incorrect” for their impact on justice. What is not presented is a choice to undo the injustice of the system itself, in which jury manipulation is permissible. Common ethical issues The rapid increase in the availablility of data, particularly on consumers and their habits, has led to the rapid increase in its manipulation and exploitation, leading in turn to ethical challenges as community dependence on the systems that gather the data has produced apathy, indifference, or a sense of powerlessness in resisting it. The very term algorithm has been re-purposed by social scientists to further add to the problem by obfuscating the issues as they attempt to take ownership of the matter. The transparency of algorithmic actions has been problematised. A new fear is whipped up about how we are being manipulated – we are, of course, and it’s our fault because we are weak, stupid or lazy. The fear of processes that are not simply understandable exacerbates the feeling of powerlessness. Explainable Artificial Intelligence, or XAI is a recent field that tries to push back against this fear. Why? Because that fear results in the populus avoiding participation in the behaviours that enslave them. “You do look glum! What you need is a gramme of soma.” (Huxley, 1955). The issue of bias in algorithms seems to be poorly understood, perhaps because of the belief of the academics that they hold some kind of moral authority to impose blind egalitarianism, or a kind a false neutrality on systems that appear to present biases. This, from the course: This is because they are designed by humans and trained on data generated from and by us, and therefore hold the potential to encode discrimination within decisions and predictions. What seems to be suppressed is the possibility that the data may lead to unpalatable conclusions. There are more blacks per capita in jail because the system is racist. Other conclusions are possible, just not acceptable, and we seem to be able to bend ourselves into all kinds of shapes to avoid them. Consent Consent is presented as a way to cleanse the abuse of data and hold harmless those that use it. This is difficult, of course and those who operate systems in my experience are often ignorant of consent requirements (in the use of submitted assignments to inform future students, for example), or simply ignore them in the hope that what the eye doesn’t see, the heart doesn’t grieve over. Consent doesn’t work. References "],["week-2-crime-and-justice.html", "Week 2: Crime and Justice", " Week 2: Crime and Justice Ambiguous Ethical Issues Crime, Justice and Technology Bias De-biasing Algorithms Fairness Data Justice "],["ambiguous-ethical-issues.html", "Ambiguous Ethical Issues", " Ambiguous Ethical Issues Predictive policing Another talking head, this time revealing the extent of the use of technology in policing in the UK, as reported by the leftist lobby group NCCL4(Couchman, 2019). Here, the media response to the use of AI to predict things like potential hot-spots for new crime, or even potential criminal behaviour of individual citizens, is noted. They made links to the Hollywood Film, “Minority Report” which was based on a similar theme. The fear is that existing (negative) biases will be amplified: this is ceratinly a resonable fear and hinges on the data that is used to train the machines, which must be historical data. “..for I the Lord thy God am a jealous God, visiting the iniquity of the fathers upon the children unto the third and fourth generation…” – Exodus 20:5 What the video fails to do is to identify this report’s finding that predictive policing is not in use (and has not been used) in Scotland or Northern Ireland. I found that odd, as the presenter described the report as being “about the rise of predictive policing across the UK.” The attraction for police forces is clear: greater efficiency in the deployment of sparse resources is a desirable feature of any publicly-funded service. Similarly, benefit and social work agencies have the same gains to make, and the different application of deciding fair and consistent sentencing in the criminal courts is also clearly desirable if it is sound. That requires awareness of the full feature set of the data used to train the systems and the ability to compensate for errors by the application of appropriate weightings or bias. Algorithms and mathematical models This section provides example cases of “algorithmic bias” in welfare services that are in part computerised. Interestingly, it doesn’t mention China’s Social Credit System which “rates” individuals according to trustworthiness: participation in the system became mandatory this year for Chinese citizens. Social Credit is economic and social reputation of individuals and business entities. Reputation is earned and lost by behaviours: good acts like giving blood or doing voluntary work are positive, bad acts like jaywalking, using your sister’s transit ID card, jumping a red light, are negative. Credit determines how accessible services and rights are to you: university places, hospital procedures, employment, and so on. Untrustworthy citizens are posted on social media channels and posters. Automating poverty task References "],["references.html", "References", " References Bardi, A., Holloway, R., Lönnqvist, J.-E., Bevington, P., Hanel, P. H. P., Maio, G. R., Soares, A. K. S., Vione, K. C., De Holanda Coelho, G. L., Gouveia, V. V., Patil, A. C., Kamble, S. V., &amp; Manstead, A. S. R. (2018). Cross-Cultural Differences and Similarities in Human Value Instantiation. https://doi.org/10.3389/fpsyg.2018.00849 Couchman, H. (. (2019). Policing by Machine (pp. 1–48). Liberty. Huxley, A. (1955). Brave new world : a novel. Penguin Books in association with Chatto &amp; Windus. Shakespeare, W. (1591). Henry VI, part 2: Entire Play. http://shakespeare.mit.edu/2henryvi/full.html Turing, A. M. (1950). Computing Machinery and Intelligence. Mind, LIX(236), 433–460. https://doi.org/10.1093/mind/LIX.236.433 Wagner, B. (2019). Ethics As An Escape From Regulation. From “Ethics-Washing” To Ethics-Shopping? In M. Hildebrandt (Ed.), Being profiled (pp. 84–89). https://doi.org/10.1515/9789048550180-016 "]]
