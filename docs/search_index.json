[["index.html", "Data Ethics, AI and Responsible Innovation Introduction", " Data Ethics, AI and Responsible Innovation Nick Hood 14 December 2020 Introduction TL;DR: I didn’t complete this course because I came upon an impossible obstacle: I lost trust in the course and the presenters in week 4. Skip to the reflection. These are my notes from participation in the Edinburgh Data Ethics MOOC, running in November and December 2020. Any errors here are my own responsibility. © 2020 Nick Hood @NixImagery This document was last updated on 14 December 2020 at 09:15. "],["course-overview.html", "Course overview", " Course overview Week 1 Ethics and Law What are ethical values? Can we rely on legal regulation? What are the most pressing issues facing data driven industries? Week 2 Crime and Justice Should we use predictive policing and sentencing algorithms? How can biases sneak into such algorithms? How can we remove them? Week 3 Home and City What are the promises of smart homes and cities? How can they impact our privacy and freedom? How can we design them to protect those values? Week 4 Money and Markets Can future tech lead to a world without money? Can algorithms help us fairly distribute resources? How could we design a fair AI? Week 5 Life and Health Should we keep genetic databanks? Would you trust an AI doctor? What are the principles of responsible research and innovation? "],["week-1-ethics-and-law.html", "Week 1: Ethics and Law", " Week 1: Ethics and Law This starts with some housekeeping, introductions to the tutors and setting up a profile within the course. Dr. Ewa Luger gets us started by considering a “broad overview” of ethics, using a radio interview about the Cambridge Analytica (CA) scandal as a resource to get us thinking before we embark on a more detailed introduction to ethics. Ethical ‘Issues’ Introduction to Ethics Legal and ethical Considerations Information, Control and Power Ethical ‘Issues’ In the clip, problematised issues with the CA matter included: the way data was gathered from people the fact that the data was then passed on to another party that the data was now available for commercial use that the data was used for political purpose the FaceBook (FB) login on an app gave permissions for all of users’ FB data to be used and shared people aren’t outraged, or even bothered by this enough to take action Discussion forum task Think back over your professional life and identify one example of an ethical issue that you personally experienced. In the discussion forum, write a brief summary (around 50-100 words) describing this issue - try to explain the context, what happened, and why you felt it was an ethical issue. After you’ve done this, comment on two other posts. I had to take a quick look at the forum first to gauge the level of ethical matters that were being shared. There seems to be a discernible difference between moral, legal and ethical matters in the revelations on the forum. My own example came from quite a selection of professional experience that ranged from legal but immoral, moral but illegal, unethical but moral and legal. I picked an example from a while back. I was project manager in a business that made flight crew training simulators. These devices were multi-million-dollar complex machines that used specially designed and manufactured circuit boards mounted in standard racks to drive the various components of the device. I had an internal review one morning that failed badly when the equipment that had been working well the night before suddenly failed to function. It was extremely embarrassing for my team. We discovered later that day that in the night, another project manager had switched a key circuit board for a faulty one from his own machine. The other examples I commented on or read seemed to me to be not so much ethical issues as difficult choices or situations. This is causing me to wonder if I have a secure understanding of the term, “ethical”. Hopefully the next section will clear that up for me. "],["the-challenge-of-data-driven-innovation.html", "The challenge of data-driven innovation", " The challenge of data-driven innovation Issues arising from the 2018 Cambridge Analaytica scandal are identified as: the value derived from data is not evenly distributed power rests in the hands of very few the potential for harm and inequality is high the potential impact of bad actors These are, in my view, issues of power arising from wealth inequity and not particularly related to anything technological. The difficulty is that technology amplifies the effects (as it amplifies inequities in a classroom – this much we have learned in moving to “Digital First” pedagogies resulting from the COVID measures). The basic principles of ethics are described as, “how we should address these issues, whilst minimising harm, ensuring morally good outcomes and fairness, and protecting human autonomy.” Data at the heart of it For the purposes of the course, this definition of ethics is set in the context of data-driven innovation which makes use of large amounts of data to train algorithms that make predictions or provide insights for decision making. Machines are increasingly now involved in the decision making too, and in taking actions. These machines are described as intelligent. Visibility of rationale These machines include, of course, neural networks and we know from the way that these pattern-matchers work, that other than the input and output layers, it is not possible to determine the path of the rationale – the specific choices ot selections being made in the intermediate steps – that underlies the outputs. Ambient intelligent systems Interactional moments are perceived to provide friction in the operation of systems, therefore designers are removing these for a smoother experience – think of entering your password every time you wanted to look at your phone, now replaced with fingerprint or face recognition. The result is that such interactions become invisible to the user: it is the environment we interact with in a natural way. Unnaturally, the environment is reading us to discern intent. Human –&gt; AI interaction This is problematised in a video clip, suggesting that this interaction is not yet defined. I rather think that Turing (1950) had a clear enough view of what it might look like, but recent innovations have increased system opacity such that understanding how the system makes choices based upon how it perceives us is difficult. In some sense, we are already familiar with this idea with our less tech-savvy relatives who need support working with machines. It gets harder when you don’t know there’s a machine. Utility Comparison with basic utilities like water, power, etc., is made from a very privileged first-world perspective: we only notice them when they fail. These systems were unregulated in their early days, too, and some of the horrors of human behaviour (Edison, for example) are long forgotten. As data-driven innovation moves to infect our lives by becoming a utility, the question is asked, “should it be regulated?” First steps have been made in this, of course, and we see the return of interactional friction with things like the cookie quiz on GDPR-compliant websites. The monetisation of data has led to the term, “surveillance capitalism”, which describes how our identity and behaviours have become the product in a high-value industry. Policy lag Clearly, policy makers are well behind the curve when it comes to data-driven innovation. GDPR and similar legislation have added friction yet advances are made at speed. The additional reading (Wagner, 2019) is critical of policy makers and suggests that ethics is what corporations do to avoid government interference. Values What we value in society is a cultural attribute. Values are therefore dependent on context and are not absolute: I have often said that every principle has a price attached to it, and that if the price is right, all principles are for sale. This has caused strong reaction when vocalised that way, especially from those who feel that they themselves hold high principles in comparison with others. Clearly, passions are aroused when deeply-held values are challenged, as the Guardian article on Peter Seeger illustrates. Examples are discussed in the course material to illustrate the point that values depend very much on context. This statement is a call to action, perhaps, talking of moral dilemmas and conflicting values: “Our job, as ethical actors, is to identify what they are, and then negotiate how best to instantiate and balance them.” Research is as robust as you’d expect for the social sciences, with this, from a study of a small sample of people (N &lt; 700) in three countries with similar (colonial) cultural pedigree: “… people endorse the same values to a similar extent across countries and also instantiate them similarly.” (Bardi et al., 2018) Not much of a surprise there, given the evolutionary path of the handful of people studied. Case studies Proejct Maven and the Google employee revolt against it is presented as the first case study of how corporations can make choices that do not align with the values of the stakeholders, in this case, the staff. That project sat in stark contrast to Google’s “Don’t be evil” mantra, still enshrined in its code of conduct (cf. the characterisation of ethics as an escape from regulation by Wagner (2019)). The military application of data-driven innovation is clearly compelling, and others are emerging, including Clearview AI’s face recognition app, which the company has worked hard to present as ethical use of technology. References "],["what-are-ethics.html", "What are ethics?", " What are ethics? A branch of moral philosophy: a codification of habits that are valued within the context of a culture: ergo, no right or wrong answers, just human custom. Normative ethics are those that define what we ought to do, without concerning themselves with what we actually do. Within that class of ethics are 3 persepctives on how we should act: Deontological ethics (rules) rules of duty and obligation – perhaps universally agreed upon, like not killing each other, or hurting animals. Some things are always right, others, always wrong, no matter the consequences. Teleological ethics (consequences) focus on the outcome of actions – actions for the greater good, perhaps, or the end justifying the means. Consequentialism is the view that the moral quality of a choice is decided solely by its outcome. Virtue ethics are about judgement of people’s character or moral fibre. The obvious issue here is that life is complicated, isn’t it? We can’t make a simple set of rules because the complexity of life requires us to make choices sometimes that go against those simple rules for a better end. Is there any point, then, in having rules in the first place? Well, clearly, because we want everyone else to make choices that don’t harm or disadvantage us. The trolley problem Scenario 1 A runaway train is travelling on a railway track towards 5 people. You can’t warn them, but there is a lever that you can operate to switch the tracks. The problem is that there is a person on the other track. Do you pull the lever or not? Scenario 2 As above, but now there are no points – instead, a large person standing a bridge over the tracks. Do you push Fatty into the path of the train to save the five? I chose yes in both scenarios. “The needs of the many outweigh the needs of the few.” – Spock Interestingly, although I was with the majority in the first scenario, I was in the minority in the second. I don’t understand this difference, except for the difference between operating a control like the lever, and physically acting on another human being – the former seems less connected to the action, perhaps. Other scenarios How does the trolley problem change when the 5 are convicted rapists? Children? When Fatty is a scientist working on a cure for Leukemia? When the 5 are Mountain People (insert your own other-class of person)? Tories? Welsh? Rednecks? Making moral decisions So, are ethics about choices, or outcomes? Rules or consequences? MIT are trying to build a picture of moral acceptability with their crowdsourcing moral machine project in which you are presented with multiple scenarios (like driverless car choices) in which you decide the path the vehicle should take, usually resulting in somebody or something dying. It is easy to become quickly abstracted from the awfulness of the choices you make in this game. Guidelines in ethical frameworks for machine choices are broadly divided into four categories: Do good Minimise harm Respect human autonomy Be just or fair Someone should tell Peugeot about the third one. My wife’s 2008 has a really irritating habit of grabbing the wheel if you change lanes without indicating, thinking that you’ve fallen asleep. I haven’t found a way of switching that off yet, but it is really disconcerting, especially on a long trip at night with no other vehicles around. Transparency of the algorithm and accountability are increasingly being emphasised. These frameworks have become regarded as inadequate as they offer a way for corporations to hide behind them in what is called, “ethics washing”. This is a problem for all rules or specifications, or checklists. Decision-making in Scottish Teacher Education In my own application, teacher education1, the GTCS Professional Standards are meant to provide an objective benchmark that describes the competencies and skills of all teachers in Scotland. We know that the application of these standards is in the hands of the profession itself and therefore wildly variable in their interpretation. The standards themselves are written in ambiguous and wishy-washy language, like most things in state-provisioned education, and so are highly subjective and open to – interpretation or abuse, depending on your view of an individual situation. “Universal law is for lackeys. Context is for kings.” –Lorca (Goldsman, 2019) Moral decision-making applied to data ethics A couple of questions are asked in a section making the links between moral decision making and data ethics. The first, “Should we require people to give their DNA to a gene bank?”, screams at me, perhaps because of my age and the closeness people of my age have, although not by first-hand experience, of the horrors of the Second World War. Everything about centralised reporting of ethnicity makes me recoil: I never provide information about my ethnicity, and I would need a very good reason before I ever did. We are forgetting: something that gets me in trouble almost every year, with very real regret and a deepening sense of injustice2. The second question relates to social media and their handling of “hate speech”. This is topical: I have just deleted my personal Facebook account3 after they, once again, dismissed my objections to posts intended to whip up hatred of muslims, or other racial or ethnic groupings. In every case I have raised, the posts have not been found to breach codes of conduct, illustrating what is described as ethical washing (see above). To my cost – I have cut off friends and family with whom I am only connected this way – I have deleted my account. A thought experiment We are tasked with making our own thought experiment to allow us to examine our beliefs and “surface” factors that influence our judgement. This is mine. The question Is it OK to kill to save lives in a war when you are not a combatant? Parameters In the context of war, it is given that the combatants of one side are allowed to kill the combatants of the other. What constitutes “combatant”, however? Is it anyone who wears the uniform, or stands behind the barricades? What about observers? The story During an African war, a soldier of an impartial peacekeeping force (soldier A) is invited to take a ride in a helicopter on a routine supply drop to a station in the bush. Soldier A has not been explicitly told that he is not allowed to ride in the vehicles of either side. The route is a well-known safe corridor, well policed by peacekeepers and respected by both sides in the war. Soldier A takes the ride, seated in the open side door of the helicopter next to a mounted cannon. En route, the helicopter comes under fire unexpectedly. The pilot takes evasive action by banking sharply to right. Soldier A can see smoke coming from a group of boulders on the ground directly below him. He realises that his life, and the lives of the others in the helicopter are in danger. He hears the pilot on the intercom shouting, “Shoot them! Shoot them!” You are soldier A. Do you use the cannon and try to save yourself and the crew? References "],["legal-and-ethical-considerations.html", "Legal and ethical considerations", " Legal and ethical considerations Legal Well, let’s not pretend that there’s much of a connection between law and morality. Laws are instruments of power. They belong to the powerful. Even at mate’s rates, a lawyer’s fees are vulgar and shameless. Even if it were accessible by the common people, legal decisions have always been historically hysterical, wildly inconsistent and woefully inadequate. It’s noticeable that the great democratisers of the law have been people like Napoleon and Hitler. It’s about time we replaced the whole system with a machine. “The first thing we do, let’s kill all the lawyers.” – Henry VI, Part 2, Act IV, Scene 2 (1591) The laws of the land are the first and greatest example of what the course describes as “ethics washing”, the practice of corporations who create “codes” to lure the public into a false sense of security and earn undeserved trust. Laws make us feel safe but do nothing to make us safe as enforcement and monitoring are lacking. In the same way that the discussion in the course speaks of ethics guidelines being used by companies “to ward off regulation by the state through formal laws”, so government uses laws to ward off revolution and disorder. There is a tension in data ethics between the need and cost of self-regulatory codes of conduct, and external regulation through the courts. The course summarises data ethics and the law: Hard ethics is needed to understand and interpret laws, and to make sure legislation is followed. Soft ethics are normative rules that tell us how to behave morally, whether the law addresses our actions or context. Compliance with the law is normally necessary for ethically correct conduct, but may not be enough for it. Ethical? Nothing ethical going on here. I skimmed the transcript of the role-play presented by the course (I am out of week already) and also the discussion on the ethics of setting cookies for the participants of ths MOOC. Much angst, I see. Tracking is an issue for me and therefore I take steps to mitigate it. I don’t allow setting of third-party cookies, and have a policy of clearing all cookies on exit from a browser session. I also use different browsers. Tracking is difficult under those circumstances: this is working for me. How do I know? When I get advertising, it’s usually totally irrelevant, which gives me some comfort, but I’m not complacent about it. I hate advertising: it’s one of the manifestations of what is truly awful about human beings. Solutions like the GDPR permissions dialogues are no better than the advertising itself. They don’t solve any problems, they just get in the way and decrease the chances of me using the sites that push them into my face. I make use of ad blockers, brutal cookie policy, and text readers when using the web. Interestingly, the link to the EC directive yields a 404 (not found) error. The EdX FAQ is a good example of the dishonest corporate response to such issues: it presents questions “frequently asked” by users which it doesn’t answer. One of the difficulties of this dominance of the common habit of making a website, for whatever reason, of the legal constraints is that the people that benefit most from this are lawyers. They get fat on the friction of common activities4. I have been making websites since the early 1980s before anyone knew what the WorldWide Web was. Most of them are pro bono educational sites, or services for the common good. Some have been for profit but not in the “YouTuber” sense: I have charged fees to compensate for the time it has taken. That time has not included exhaustive compliance checking with the various laws and expectations. I have always worked on the basis that I haven’t got any money, so am not worth pursuing in litigation. So far, so good. At no point has it been made clear why cookies are necessary. Disabling cookies on the EdX site results in denial of access immediately on refreshing the page. So, the first purpose is to grant access to the site (which can be done in other ways easily enough, and more securely). References "],["information-control-and-power.html", "Information, control and power", " Information, control and power Formalism Legal choices based on logic only: what you did, and what the law is. Realism Your conviction depends to a large extent on what the judge ate for breakfast. A task is given in which students are asked to decide if building an app to influence proceedings based upon what is known from an AI analysis of jury members, is ethical. I picked the red envelope, but both answers are characterised as “incorrect” for their impact on justice. What is not presented is a choice to undo the injustice of the system itself, in which jury manipulation is permissible. Common ethical issues The rapid increase in the availablility of data, particularly on consumers and their habits, has led to the rapid increase in its manipulation and exploitation, leading in turn to ethical challenges as community dependence on the systems that gather the data has produced apathy, indifference, or a sense of powerlessness in resisting it. The very term algorithm has been re-purposed by social scientists to further add to the problem by obfuscating the issues as they attempt to take ownership of the matter. The transparency of algorithmic actions has been problematised. A new fear is whipped up about how we are being manipulated – we are, of course, and it’s our fault because we are weak, stupid or lazy. The fear of processes that are not simply understandable exacerbates the feeling of powerlessness. Explainable Artificial Intelligence, or XAI is a recent field that tries to push back against this fear. Why? Because that fear results in the populus avoiding participation in the behaviours that enslave them. “You do look glum! What you need is a gramme of soma.” (Huxley, 1955). The issue of bias in algorithms seems to be poorly understood, perhaps because of the belief of the academics that they hold some kind of moral authority to impose blind egalitarianism, or a kind a false neutrality on systems that appear to present biases. This, from the course: This is because they are designed by humans and trained on data generated from and by us, and therefore hold the potential to encode discrimination within decisions and predictions. What seems to be suppressed is the possibility that the data may lead to unpalatable conclusions. There are more blacks per capita in jail because the system is racist. Other conclusions are possible, just not acceptable, and we seem to be able to bend ourselves into all kinds of shapes to avoid them. Consent Consent is presented as a way to cleanse the abuse of data and hold harmless those that use it. This is difficult, of course and those who operate systems in my experience are often ignorant of consent requirements (in the use of submitted assignments to inform future students, for example), or simply ignore them in the hope that what the eye doesn’t see, the heart doesn’t grieve over. Consent doesn’t work. References "],["week-2-crime-and-justice.html", "Week 2: Crime and Justice", " Week 2: Crime and Justice Ambiguous Ethical Issues Crime, Justice and Technology Bias De-biasing Algorithms Fairness Data Justice "],["ambiguous-ethical-issues.html", "Ambiguous Ethical Issues", " Ambiguous Ethical Issues Predictive policing Another talking head, this time revealing the extent of the use of technology in policing in the UK, as reported by the leftist lobby group NCCL5(Couchman, 2019). Here, the media response to the use of AI to predict things like potential hot-spots for new crime, or even potential criminal behaviour of individual citizens, is noted. They made links to the Hollywood Film, “Minority Report” which was based on a similar theme. The fear is that existing (negative) biases will be amplified: this is ceratinly a resonable fear and hinges on the data that is used to train the machines, which must be historical data. “..for I the Lord thy God am a jealous God, visiting the iniquity of the fathers upon the children unto the third and fourth generation…” – Exodus 20:5 What the video fails to do is to identify this report’s finding that predictive policing is not in use (and has not been used) in Scotland or Northern Ireland. I found that odd, as the presenter described the report as being “about the rise of predictive policing across the UK.” The attraction for police forces is clear: greater efficiency in the deployment of sparse resources is a desirable feature of any publicly-funded service. Similarly, benefit and social work agencies have the same gains to make, and the different application of deciding fair and consistent sentencing in the criminal courts is also clearly desirable if it is sound. That requires awareness of the full feature set of the data used to train the systems and the ability to compensate for errors by the application of appropriate weightings or bias. Algorithms and mathematical models This section provides example cases of “algorithmic bias” in welfare services that are in part computerised. Interestingly, it doesn’t mention China’s Social Credit System which “rates” individuals according to trustworthiness: participation in the system became mandatory this year for Chinese citizens. Social Credit is economic and social reputation of individuals and business entities. Reputation is earned and lost by behaviours: good acts like giving blood or doing voluntary work are positive, bad acts like jaywalking, using your sister’s transit ID card, jumping a red light, are negative. Credit determines how accessible services and rights are to you: university places, hospital procedures, employment, and so on. Untrustworthy citizens are posted on social media channels and posters. Automating poverty task Time is very short this week, so I skimmed the articles but did not participate in the crowdsourcing discussions6. References "],["crime-justice-and-technology.html", "Crime, Justice and Technology", " Crime, Justice and Technology Ferguson (2017) describes how policing moved from a clinical to an actuarial model: from expert, individual decisions unconstrained by the parameters of a pre-designed model, to fitting people and their behaviours into categories derived from historical anaylsis. This is clearly a fundamental data mistake: to close off a model to new experience and insight is an accountant’s blunder7. Justification for the use of such tools is offered within the terms of a social contract, in which citizens surrender certain freedoms8 in exchange for the benefit of protection against others who might harm us. Two types of predictive policing are identified in the course: Predictive mapping Individual risk assessment programmes Predictive mapping This relates data on the time and place of crimes, perhaps also with additional data on the type of crime (but not the criminal) and uses this to make a prediction or forecast of likely “hot-spots” to which police resources may be deployed in an attempt to mitigate. There are a number of reports cited for further reading but the report from Chile (Contreras, 2019) is not atypical of much media coverage of the use of this kind of technology. Here’s how it opens: El 29 de agosto de 1997, a las 2:14 AM Skynet toma conciencia de sí misma. Skynet es una inteligencia artificial que lidera el ejército de las máquinas que quieren exterminar a los humanos pues los considera una amenaza para su propia supervivencia. Skynet está basada en una red neuronal que funciona en la nube y que maneja todos los aviones y armas no tripuladas de los Estados Unidos de Norteamérica. Para eliminar a los seres humanos desata una guerra nuclear y el posterior apocalipsis. On August 29, 1997, at 2:14 AM Skynet becomes aware of itself. Skynet is an artificial intelligence that leads the army of machines that want to exterminate humans because it considers them a threat to its own survival. Skynet is based on a neural network that operates in the cloud and handles all the planes and unmanned weapons in the United States of America. To eliminate humans it triggers a nuclear war and subsequent apocalypse. – translation by DeepL Individual risk assessment programmes Worse than that, of course, is where it gets personal: connecting individual with risk assessment is a hot potato in education, let alone policing. Perhaps because it starts with the stance that an individual person presents a risk to “us”, making them implicitly “other”. One of the difficulties with predictive policing is that it undermines one of the purposes of criminal law, rehabilitation, because it targets previous offenders for closer police attention in their community. This must also significantly impact on deterrence: if the police are focusing on the ex-cons, they are less likely to be looking at the rest of us, increasing the likelihood that we will be deterred from criminal activity9. The LAPD stopped using its LASER tool in 2019 but continues to engage in data-driven policing. Sentencing and parole The use of data is not a new feature of sentencing and parole decisions world wide and these uses have led to sustained bias scoring (and thereby sentencing or incarceration decisions). References "],["bias.html", "Bias", " Bias The course resorts to Webster’s, of all places, to find a definition of bias. My background understands the term as: “A steady voltage or current applied to an electronic device” – (bias, adj., n., and adv. : Oxford English Dictionary, n.d.) For me, bias and its sister, discrimination are not inherently Bad Things. The ordering of the meanings of the word in these dictionaries offers perhaps a cultural explanation for a bias of the course writers, who seem to be seeking to associate injustice with this word. The OED’s principal meaning of bias is slanting, oblique (ibid.). Algorithmic bias One problem with the current criticism of “algorithmic bias” is that the scapegoat is either the algorithm itself, or the programmers that make it. It seems unfashionable to respond, “fair comment” when outcomes of data analysis go against our modern, fragile, sensibilities: “Google’s ad-serving system showed an ad for high-paying jobs to men much more often than it did for women” – from CMU research, cited in Kirkpatrick (2016) Clearly the targeting of the ad wasn’t based upon the single variable of gender: other factors are significant in the selection and are ignored in the reporting to make a more sensational commentary. This is the real difficulty for me here: that we ignore the manipulation of our responses by irresponsible and lucrative10 articles like these. Big Data and implicit bias A reading from this week (Barocas, 2014) also examines the meaning of words when suggesting ways of tackling the problem of implicit bias in algorithms derived from historical data. I like the moderate language used in this essay: “If data miners are not careful, the process can result in disproportionately adverse outcomes concentrated within historically disadvantaged groups in ways that look a lot like discrimination.” – Barocas (2014), p. 673 “Discrimination may be an artifact of the data mining process itself, rather than a result of programmers assigning certain factors inappropriate weight” – ibid. p. 674 So, we can step away from the tribal frowning at the technology and the people who program it and recognise that our own past behaviours have created this learned behaviour, in the same way that it is created in our own attitudes and biases. Barocas (2014) suggests this can be done via the lens of (US) Title VII (Civil Rights Act of 1964 - CRA - Title VII - Equal Employment Opportunities - 42 US Code Chapter 21, 1964) and by doing what I consider to be the obvious, which is to understand how these algorithms get their biases in the first place: “Data mining takes the existing state of the world as a given and ranks candidates according to their predicted attributes in that world.” – Barocas (2014), p. 731, orginal emphasis References "],["de-biasing-algorithms-and-fairness.html", "De-biasing Algorithms and fairness", " De-biasing Algorithms and fairness Fairness-aware machine learning is a term used in an EU report (Tolan, 2018) that asserts that fairness in this sense depends on the domain in which the model is being made and therefore the fairness constraints applied should be specific to that domain. The group fairness approaches in the EU report apply political or popular biases like: “the share of defendants classified as high risk should be equal across different protected groups” – Tolan (2018), p8 This is called demographic parity and tries to neutralise an aspect of the source data: suppressing one truth in the name of another, perhaps. In another, calibration is applied such that “the proportion of people re-offending is the same across protected groups” (ibid, p.10), and to achieve similar people should be treated similarly is considered a non-trivial task on account of deciding on what data is required to identify how similar two people are. “Fairness through unawareness” is one method described by which algorithms might be made to mitigate bias: this seems to take us full circle back to where the problem began, in which the use of historical data is used to train the model from the prior behaviours which themselves have included hidden biases. References "],["data-justice.html", "Data Justice", " Data Justice The framework called Data Justice looks at both social and technical aspects of machine bias. The idea is to oppose exacerbating social injustice by the rapid adoption of technologies that embed data which itself may include past social injustice. The course points us again at Couchman (2019) for its stance on “policing by machine” but also introduces us to one writer’s proposal for redressing the power balance back in favour of the citizen, namely (in)visibility, (dis)engagement with technology and antidiscrimination (Taylor, 2017). The first of these calls for greater transparency of what and how data is collected and used, and allows individuals to choose not to be part of it. The second relates to the latter point and calls for greater control for the individual on how (or whether) they participate in the data markets. Finally, individuals should have the right to call out bias or unfair treatment at the hands of data-informed systems. This, I think is particularly important for public services. Increasingly, watchdog groups are being established around the world to raise awareness and facilitate action against the abuse of data. The UK’s Data Justice Lab sits within Cardiff University’s media school. Interestingly, the FAT/ML11 website hasn’t been updated for the past two years, which might suggest that nothing much is happening in the group. References "],["week-3-home-and-city.html", "Week 3: Home and City", " Week 3: Home and City The Internet of Things The Smart Home The Smart City Design solutions "],["the-iot.html", "The IoT", " The IoT The Internet of Things is growing rapidly worldwide and quite possibly more so under the pandemic because of the shift to online shopping it has driven. Industrial infrastructure is enhanced by data and the greater data association with objects. According to IoT Analytics (a market research firm), manufacturing leads in the application of these technology advances. (IoT Analytics, 2020). Top 10 IoT application areas Smart cities are the stimulus for the first discussion in the forum this week. A video clip sets out the possibilities and the pitfalls12. Activity: what do you think are the biggest concerns raised by the Internet of Things? For me, there is clearly the potential for the abuse of power. Surveillance by digital means is less visible but no less terrifying than Goebbels’ yellow badge. Hammer Chewers A further video introduces further the IoT using a “two people pretend to interview each other” approach which is a bit cheesy, especially as neither are natural presenters. I’m finding the pedagogy of this course fascinating: for a University which has a School of Education, it’s eye-rolling (face-palming?) to see academics who think they know how to teach, actually try. To be fair, it’s not unique to Edinburgh: this problem presents itself in other courses I have seen. It’s a delighfully ironic example of Dunning-Krueger (Kruger &amp; Dunning, 1999). So, what is it? Imagine the world in the age of the new telegraph. Messages can be sent from place to place, if those places are connected by wires. Add to this, the technological advances in sensing the environment, and we have telemetry. The ability and desire to save our legs has been the driver of innovation: the Trojan Room Coffee Pot is a legendary modern example. Now, the Internet provides the wires for communication, and miniaturisation and scaling of sensors provides a deluge of data. The IoT is not a new thing at all, merely the development of something much older. It is telemetry, but with the added bonus of aggregation and inference: analysis of increasing amounts of data from the physical world yields insights we didn’t have before. What makes it interesting is the development of the neural network and the inferences they can make from many data points. Inferences that can aid policy and management decisions (like, “do we need street lights on in this district between 2 am and 4 am?”). Inferences like “these districts are more prone to drug crime”. What is described as the datafication of the environment is surely an exaggeration, but the more we know about the world, then so must our choices be improved? The problems arise when data is re-purposed. This problem is also identified as function creep. Steps are being taken now to make it harder for this re-purposing of data to happen without the provider of the data having informed awareness, and giving their informed consent for it to be used in a new or different way than was understood. “Doing data protection by design and default” is mentioned in the video to suggest that limits can be placed on the use of the data when innovation is taking place. References "],["the-smart-home.html", "The Smart Home", " The Smart Home Cartoon Listening © XKCD The situation is set out for us: we are using technologies, sometimes without having the right to refuse, which are gathering data on us, our homes, and our lives. The ethical concerns associated with these innovations are characterised as being of two kinds: Social and cultural concerns; and Governance concerns. Audit and share Students on the course are invited to consider and share their thoughts on the risks and benefits of smart devices. We are challenged to consider the security measures we (may) have taken to protect privacy as we use them. I did an audit of my home and consider it not to be so smart: we have wifi, of course, with an up to date firewall and port management; an Amazon Fire Stick on one of the stupid TVs to make it smarter; an Amazon Echo Dot that is just hilarious13, that we got free with the Hive device for the boiler. Governance Laws like GDPR address concerns like opaque data flows, where data is moved around and shared within systems without the awareness of the people using those systems. These laws try to ensure that users are informed about these data flows and have some sense of choice, although it is evident that these mechanisms fail: they are themselves opaque and inaccessible to users. A modern defence is obfuscation – burying it with bureaucracy – and one which organisations use extensively to hold up a veil of compliance and transfer blame to the participants or users who are too lazy to fill the forms in, or read the Ts &amp; Cs14. Academics write that privacy involves protecting the context of data as much as the data itself. Examples activity We are encouraged to “share and comment” on a Padlet articles relating to four examples: internet-connected home systems; smart homes for the elderly; Alexa/Echo; and smart meters15. Try the extreme farts extension pack for Alexa.↩︎ This particular problem is causing me considerable difficulty at the moment. Systems purported to support staff and students are in fact stressors because they present a wall of undifferentiated administration where none is required, or at least, a minimal set and some principles that, once clearly stated, are all those staff and students need to decide for themselves how to comply with requirements. It’s incompetence but because it is presented as “action in support”, nothing is done about it and those who speak out are seen to be oppugnant. One has empathy with Lord Altrincham.↩︎ Apart from my hatred of Padlet and similar “digital equivalents to something useful”, I am too short of time for this. The time estimate for this course is way off the 3 hours a week for a full engagement with it and I am a little frustrated by not being able to dive into the wider reading. I really wanted to look at Danah Boyd’s paper on teenagers’ engagement with technology but that will have to just pass me by. This is not a criticism of the course, but of my own stupidity in engaging with it at this time.↩︎ "],["the-smart-city.html", "The Smart City", " The Smart City This section begins with some illustrations of “smart” city projects that have fallen on their faces through inadequate understanding of the people they pretend to be for. It seems that the planners’ model of urban society is hugely over-simplified in these examples, which in turn is the cause of their failure to flourish. Investors and citizens alike seem to be highly circumspect when it comes to controlled surveillance. This, notwithstanding the success of infection-beating systems rolled out to combat the spread of SARS-CoV-2. A couple of lazy “discuss it amongst yourselves” tasks ask us to tell each other what it’s like in your country and to stand in moral judgment of the “millennial app” Waze (I’m paraphrasing the discussions I read). Much of it was of the quality of an average FaceBook spat between rednecks and liberals. What strikes me is the length of time it has taken for society to adopt the technologies we have had for decades: I was using GIS and SGML in the 1980s and yet it has taken until the 2020s for the potential to be noticed, yet alone realised. The Chinese aren’t too squeamish to roll their sleeves up and get stuck in whilst the rest of the world pauses as the edge of the pool pondering how cold the water might be or if there might be crocodiles. I admire them for that. It will take a significant shift away from individualistic liberal democracy to make much use of these possibilities. For smart city initiatives to succeed, those creating the infrastructure and new smart services need to meaningfully engage with users and find mechanisms to have their participation in the design process. The Design Solutions section of this week’s course opens with the above statement: a flat assertion of a dogmatic liberal stance, made with neither apology nor justification. We have already seen that people don’t care about privacy until it results in some perceived injustice. They care even less when the systems are transparent: they care when the conveniences they have become used to are denied. Facial recognition How far is too far, though? The course asks whether facial recognition technology and its use by police is acceptable. Protesters are worried that it has no precedent because the term, “facial recognition” doesn’t appear in Statute and that this makes its use unregulated and unlimited. This appears to be a false argument, because the technology is merely amplifying the existing capacity of the police to recognise known persons of interest. What does seem very British in one of the clips is the fining of one poor man who covered his face up in a surveillance trial in London and landed himself a £90 fine. They wouldn’t have slapped that penalty on a woman wearing a burqa – this seems to be an example of the police abusing the “disorderly conduct” laws in the same way that any British Army NCO can abuse Section 19 of the Armed Forces Act 2006 (formerly Section 69 of the Army Act 1955), “conduct prejudicial to good order and discipline”16. What’s the beef, though? There’s a difference between getting all itchy about being recognised in a public place, and on private property. One of our great freedoms is to be able take a photograph of people in public. Many people who have become used to some sense of a “right to privacy” in recent times will take offence (for surely offence is always taken, never given) if you snap them on the high street. I don’t see why. Ben Brayne, one time RSM at Depot REME, Arborfield, famously jailed both his dog and his pace stick for misdemeanours on the parade ground under Section 69.↩︎ "],["design-solutions.html", "Design Solutions", " Design Solutions This is what this course aims to do: encourage you to reflect how your design choices can shape your users and society. I think I am beginning to see the aims and purposes of this course more clearly: to help us see what is in plain sight and already in our experience, although forgotten. Systems are powerful in that they influence behaviour of those who interact with them. Culture is a system in this regard, therefore. Societal norms have aye been manipulated by those who seek power: the information media of the time have been crucial in adjusting those norms. Today’s situation is no different: truth is hidden to enhance the message, whether for marketing or political ends. The defence is simple enough: to be informed, and to be educated17. Designing for values Lessig describes constitution as “not just a legal text but a way of life – that structures and constrains social and legal power, to the end of protecting fundamental values” (Lessig (2006), p.19). The characterisation of “code is law” as originally presented (Lessig, 2000) and in the more recent version (Lessig, 2006), interestingly dedicated to Wikipedia, underlines the point above: the parts of a system manifest and make its character: the course notes suggest that because of this, by whom and how systems are built are not just technical matters but rather “societal, ethical and political”. Values must be considered by the designers of a system: the example of a laptop camera with a built-in privacy shutter is presented to illustrate this. GDPR tries to mandate this because users are incapable of understanding the systems they are using and make choices to protect themselves from using systems that may be harmful to them. A difficulty arises when such systems replace services which might be considered basic needs: for example, it is a legal right to know how your pay packet has been calculated and yet employers hide this information behind access barriers like online pay systems. Pension providers, too, require their pensioners to be computer literate, online and savvy in order to access their own pensions. Today’s letter from USS advises that “in the future, all your documents will be online 24/7 – no more lost paper”. These abuses of power don’t even try to be honest about the reasons for such a move: and the designers of that system will have a set of values to embed within their system that have nothing to do with the needs of their customers. Even writing that sentence is difficult because the customer (I was going to write client) is no longer the person for whom the service is providing. Faced with such powerlessness in the face of innovation, the individual is unable to defend themselves against the abuses of it. Government regulation is not much more than ethics washing. Making improvements My post, under the heading “Education”, suggests this: “People are more likely to make more circumspect use of smart tech if they know what is does, what is might do, and how to prevent it. A simple example is setting your browser to block third-party cookies, employing an ad blocker, and similar tools.” References "],["week-4-money-and-markets.html", "Week 4: Money and Markets", " Week 4: Money and Markets The introduction to this week starts out with a new context: the algorithmic economy. Three questions offer a framework for our enquiry, presented as “fundamental ethical questions”. The first, in my view, is not an ethical question at all but a political one, that of how wealth ought to be distributed. Either I still am not understanding the meaning of the word, ethics, or there is again revealed here the liberal dogma of the course-writers18. “The inherent vice of capitalism is the unequal sharing of blessings. The inherent virtue of Socialism is the equal sharing of miseries.” – Churchill (1945) The other two ask questions which seem bizarre to me. One, What makes algorithms that allocate benefits and harms, fair? introduces the concept of fairness in a place I wouldn’t have expected it. Extending Churchill, life isn’t fair, so why should we break our necks with angst over our technical systems? Perhaps this is a matter of degree: we none of us particularly would disadvantage a citizen in, say, the allocation of health benefits for the colour of their skin. The final question seems also to be naïve, What are the principles that tech giants apply when they influence how our attention and income should be distributed?, the answer to which is simple and obvious enough: maximize the shareholders’ ROI. Professor Michael Rovatsos in his introduction to the week describes as utopian, a vision of the future which sounded to me more like a chapter from Huxley (1955) in its horror. I have to wonder who funds his research. He offers a dichotomy between “digital communism” and “surveillance capitalism”. He goes on, citing this as principle: “From each according to his ability, to each according to his needs!” – Marx (1875) References "],["and-so-it-goes-vonnegut1969.html", "And so it goes (Vonnegut, 1969)", " And so it goes (Vonnegut, 1969) At this point, I feel deeply uncomfortable with this course and the intentions of the authors. It is for me now nothing more than propaganda dressed as academic disinterest. I’m going to stop the course here. References "],["a-reflection.html", "A reflection", " A reflection I can’t tell you how disappointed I am in this course19. It is cack-handed and lazy, pedagogically speaking, but worse, much worse than that, it is a pretence, either ignorant and naïve, or with political intent. It feigns academic authority: it is presented by learned people keeping straight faces in the dispassionate, disconnected, and disinterested way that proper scientists do. However, early in the course the unexpected amnesia appears, followed quickly by the implicit claim to moral authority by the course writers. This is either just very bad science or supreme arrogance. Further examples20 chip away at what is left of the credibility of this course until the great reveal of the Marxist meme, offered as a principle, no less. For me, at this point, trust is betrayed, not just in the course, but also the team that present it, and perhaps even the University and bodies they represent. This is disappointing. Will I be back for another MOOC? Not likely, and certainly not another University of Edinburgh one. If I need professional teaching, then I will get a professional teacher in, or do it myself, like the plumbing. A wider problem I occasionally refer to “the socialist republic of Moray House” when observing the left-wing stance of colleagues in the part of the University I know best. It seems that such socialist leanings may not be confined to the social sciences, although this course is more social science than it is real science, and therein, maybe, lies the problem. Despite warnings (Horgan, 2013), social researchers still wade into deep waters where they are not equipped to go: morality and ethics are possibly the last place they should be allowed to wield their sickles in case they cut themselves. References "],["references.html", "References", " References Asimov, I. (1991). Foundation. Bantam Books. Bardi, A., Holloway, R., Lönnqvist, J.-E., Bevington, P., Hanel, P. H. P., Maio, G. R., Soares, A. K. S., Vione, K. C., De Holanda Coelho, G. L., Gouveia, V. V., Patil, A. C., Kamble, S. V., &amp; Manstead, A. S. R. (2018). Cross-Cultural Differences and Similarities in Human Value Instantiation. https://doi.org/10.3389/fpsyg.2018.00849 Barocas, S. (2014). Big Data’s Disparate Impact. California Law Review, 104(671), 671–732. bias, adj., n., and adv. : Oxford English Dictionary. (n.d.). Retrieved November 16, 2020, from https://www.oed.com/view/Entry/18564?result=1&amp;rskey=pOoRi9&amp; Churchill, W. (1945). DEMOBILISATION. https://api.parliament.uk/historic-hansard/commons/1945/oct/22/demobilisation#column_1704 Civil Rights Act of 1964 - CRA - Title VII - Equal Employment Opportunities - 42 US Code Chapter 21. (1964). https://finduslaw.com/civil-rights-act-1964-cra-title-vii-equal-employment-opportunities-42-us-code-chapter-21 Contreras, D. V. (2019). Batallas 3.0: Inteligencia Artificial y algoritmos versus delincuencia en Chile. https://www.theclinic.cl/2019/07/25/batallas-3-0-inteligencia-artificial-y-algoritmos-versus-delincuencia-en-chile/ Couchman, H. (2019). Policing by Machine (pp. 1–48). Liberty. Ferguson, A. G. (2017). Predictive policing. Washington University Law Review, 94(5), 1109–1189. Goldsman, A. (2019). \"Star Trek: Discovery\" Context Is for Kings (TV Episode 2017). CBS Television Studios. https://www.imdb.com/title/tt5835714/?ref_=ttep_ep3 Horgan, J. (2013). Is \"Social Science\" an Oxymoron? Will That Ever Change? https://blogs.scientificamerican.com/cross-check/is-social-science-an-oxymoron-will-that-ever-change/ Huxley, A. (1955). Brave new world : a novel. Penguin Books in association with Chatto &amp; Windus. IoT Analytics. (2020). Top 10 IoT applications in 2020 - Which are the hottest areas right now? https://iot-analytics.com/top-10-iot-applications-in-2020/ Kirkpatrick, K. (2016). Battling algorithmic bias. Communications of the ACM, 59(10), 16–17. https://doi.org/10.1145/2983270 Kruger, J., &amp; Dunning, D. (1999). Unskilled and unaware of it: How difficulties in recognizing one’s own incompetence lead to inflated self-assessments. Journal of Personality and Social Psychology, 77(6), 1121–1134. https://doi.org/10.1037/0022-3514.77.6.1121 Lessig, L. (2006). Code Version 2.0 (p. 410). Basic Books. Lessig, L. (2000). Code is Law – On Liberty in Cyberspace. https://www.harvardmagazine.com/2000/01/code-is-law-html Marx, K. (1875). Critique of the Gotha Programme. https://www.marxists.org/archive/marx/works/1875/gotha/ch01.htm Shakespeare, W. (1591). Henry VI, part 2: Entire Play. http://shakespeare.mit.edu/2henryvi/full.html Taylor, L. (2017). What is data justice? The case for connecting digital rights and freedoms globally. Big Data and Society, 4(2), 1–14. https://doi.org/10.1177/2053951717736335 Tolan, S. (2018). JRC Digital Economy Working Paper 2018-10 Fair and Unbiased Algorithmic Decision Making : Current State and Future Challenges. December. Turing, A. M. (1950). Computing Machinery and Intelligence. Mind, LIX(236), 433–460. https://doi.org/10.1093/mind/LIX.236.433 Vonnegut, K. (1969). Slaughterhouse-five, or, The children’s crusade : a duty-dance with death. Dell. https://archive.org/details/slaughterhousefivonn00vonn Wagner, B. (2019). Ethics As An Escape From Regulation. From “Ethics-Washing” To Ethics-Shopping? In M. Hildebrandt (Ed.), Being profiled (pp. 84–89). https://doi.org/10.1515/9789048550180-016 "]]
